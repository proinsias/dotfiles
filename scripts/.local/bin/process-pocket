#!/usr/bin/env python

import os

import bs4
import requests

headers={'X-Accept': 'application/json'}

print("Loading articles from medium reading list...")

url = "https://medium.com/@proinsias/list/predefined:fd6fd6e89f7a:READING_LIST"

response = requests.get(url, headers=headers)


soup = BeautifulSoup(response.content, 'html.parser')

urls = []
# Find all anchor tags with 'a' and class 'link' within the reading list
for anchor_tag in soup.find_all('a', class_='link'):
    url = anchor_tag.get('href')
    urls.append(url)

print(urls)

import sys
sys.exit()

def filterurl(url, char):
    ''' Function to prune off extra URL options '''
    try:
        return url[:url.index(char)]
    except ValueError:
        return url


# Set your Pocket API credentials.
consumer_key = os.getenv("POCKET_CONSUMER_KEY")
access_token = os.getenv("POCKET_ACCESS_TOKEN")

url = "https://getpocket.com/v3/get"
unique_urls = set()
duplicate_article_ids = []
duplicate_article_urls = []

# If an article is in the archived AND unread list,
# we want to deduplicate the unread version.
# So let's start with the list of archived articles.

print("Retrieving the list of archived articles from Pocket...")
data = {
    "consumer_key": consumer_key,
    "access_token": access_token,
    "state": "archive",  # Retrieve only archived articles
    "sort": "newest",  # Sort articles by newest first
    "detailType": "complete"  # Retrieve complete article details
}

response = requests.post(url, data=data, headers=headers)
articles = response.json()["list"]

print("Removing duplicate archived articles based on URLs...")
for article_id, article_data in articles.items():
    full_article_url = article_data["given_url"]
    # Remove extra crap from URLS (DANGEROUS - don't remove too much!)
    article_url = filterurl(full_article_url, '?utm')

    if article_url in unique_urls:
        duplicate_article_ids.append(article_id)
        duplicate_article_urls.append(full_article_url)
    else:
        unique_urls.add(article_url)

print("Retrieving the list of unread articles from Pocket...")
data = {
    "consumer_key": consumer_key,
    "access_token": access_token,
    "state": "unread",  # Retrieve only unread articles
    "sort": "newest",  # Sort articles by newest first
    "detailType": "complete"  # Retrieve complete article details
}

response = requests.post(url, data=data, headers=headers)
articles = response.json()["list"]

print("Removing duplicate unread articles based on URLs...")
for article_id, article_data in articles.items():
    full_article_url = article_data["given_url"]
    # Remove extra crap from URLS (DANGEROUS - don't remove too much!)
    article_url = filterurl(full_article_url, '?utm')

    if article_url in unique_urls:
        duplicate_article_ids.append(article_id)
        duplicate_article_urls.append(full_article_url)
    else:
        unique_urls.add(article_url)

if len(duplicate_article_ids) > 0:

    print("Deleting duplicate articles from Pocket...")
    delete_url = "https://getpocket.com/v3/send"
    delete_data = {
        "consumer_key": consumer_key,
        "access_token": access_token,
        "actions": [{"action": "delete", "item_id": article_id} for article_id in duplicate_article_ids]
    }
    
    delete_response = requests.post(delete_url, json=delete_data)
    
    # Print the number of deleted duplicate articles
    deleted_count = delete_response.json()["action_results"].count("deleted")
    print(f"Deleted {deleted_count} duplicate articles.")
    
    for article_url in duplicate_article_urls:
        print(article_url)

else:
    print("No duplicate articles!")



# If necessary, I can update this to get the access token via script as follows:
# https://gist.github.com/Mierdin/0996952ba02d87175f3b
# https://gist.github.com/alexpyoung/7e241a8f3f805630f0f66a1cf0763675#file-pocket_import-L71

# Otherwise use this service: http://reader.fxneumann.de/plugins/oneclickpocket/auth.php
    